{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "dotZjUvecRI8",
    "outputId": "a7869b66-c61a-439e-86b3-110bd10f4b0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.7.0\n",
      "Hub version:  0.12.0\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bert\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import  Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TensorFlow Version:\",tf.__version__)\n",
    "print(\"Hub version: \",hub.__version__)\n",
    "pd.set_option('display.max_colwidth',1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-uH1zw2Pa13m"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/dataset/4af304c0f797e3b08f22895d6a0dcf95eee4c37f7a20775c7a4ee2281c6ba2ff\n",
    "DATASET_PATH = \"NoThemeTweets.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv( \n",
    "         DATASET_PATH,\n",
    "         engine=\"python\", \n",
    "         encoding=\"latin1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>query_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1031761728445530112</td>\n",
       "      <td>@Tixaa23 14 para eu ir :)</td>\n",
       "      <td>Tue Aug 21 04:35:39 +0000 2018</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1031761040462278656</td>\n",
       "      <td>@drexalvarez O meu like eu jÃ¡ dei na Ã©poca :)</td>\n",
       "      <td>Tue Aug 21 04:32:55 +0000 2018</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1031760962372689920</td>\n",
       "      <td>Eu sÃ³ queria conseguir comer alguma coisa pra poder dormir :)</td>\n",
       "      <td>Tue Aug 21 04:32:37 +0000 2018</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1031760948250456066</td>\n",
       "      <td>:D que lindo dia !</td>\n",
       "      <td>Tue Aug 21 04:32:33 +0000 2018</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1031760895985246208</td>\n",
       "      <td>@Primo_Resmungao Pq da pr jeito!!Ã© uma \"oferta\", ha q aproveitar. :P</td>\n",
       "      <td>Tue Aug 21 04:32:21 +0000 2018</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id  \\\n",
       "0  1031761728445530112   \n",
       "1  1031761040462278656   \n",
       "2  1031760962372689920   \n",
       "3  1031760948250456066   \n",
       "4  1031760895985246208   \n",
       "\n",
       "                                                              tweet_text  \\\n",
       "0                                              @Tixaa23 14 para eu ir :)   \n",
       "1                        @drexalvarez O meu like eu jÃ¡ dei na Ã©poca :)   \n",
       "2         Eu sÃ³ queria conseguir comer alguma coisa pra poder dormir :)   \n",
       "3                                                     :D que lindo dia !   \n",
       "4  @Primo_Resmungao Pq da pr jeito!!Ã© uma \"oferta\", ha q aproveitar. :P   \n",
       "\n",
       "                       tweet_date sentiment query_used  \n",
       "0  Tue Aug 21 04:35:39 +0000 2018  Positivo         :)  \n",
       "1  Tue Aug 21 04:32:55 +0000 2018  Positivo         :)  \n",
       "2  Tue Aug 21 04:32:37 +0000 2018  Positivo         :)  \n",
       "3  Tue Aug 21 04:32:33 +0000 2018  Positivo         :)  \n",
       "4  Tue Aug 21 04:32:21 +0000 2018  Positivo         :)  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset[[\"tweet_text\", \"sentiment\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_text    0\n",
       "sentiment     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785814, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HmE5-_RebmIZ"
   },
   "outputs": [],
   "source": [
    "def get_treated_data(dataset, cols, cols_drop = [], col_to_change='sentiment', val_col_change = {\"Negativo\": 0, \"Positivo\":1}):\n",
    "    \n",
    "    # # 1. Criar a variável \"data\"\n",
    "    dataset = pd.read_csv( \n",
    "         DATASET_PATH,\n",
    "         engine=\"python\", \n",
    "         encoding=\"latin1\"\n",
    "    )\n",
    "        \n",
    "    # 2. Rename columns\n",
    "    dataset.columns = cols\n",
    "    \n",
    "    # 3. Drop columns not needed\n",
    "    dataset.drop(cols_drop, axis=1, inplace=True)\n",
    "    \n",
    "    # 3.1 Drop all rows with at least one element is missing\n",
    "    dataset.dropna()\n",
    "    \n",
    "    # 4. Convert setiments from \"Negative/Positive\" to \"0/1\" \n",
    "    # dataset.replace({col_to_change: val_col_change}, inplace=True)\n",
    "    \n",
    "    # Return our dataset\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "yTuXagdDtq1v",
    "outputId": "db559903-7865-44fe-b338-3e6329440d7b"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1MTnRZMubw6A"
   },
   "outputs": [],
   "source": [
    "default_cols = [\"id\", \"text\", \"date\", \"sentiment\", \"query\"];\n",
    "default_drop_cols = [\"id\", \"date\", \"query\"]\n",
    "# default_cols = [\"sentiment\", \"text\"];\n",
    "# default_drop_cols = [\"id\", \"date\", \"query\"]\n",
    "\n",
    "# class_names = ['Negativo', 'Positive']\n",
    "df = get_treated_data(df, default_cols, cols_drop = default_drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "5GbNMjbS_b8J",
    "outputId": "121fb657-1e3d-4095-c356-d209c263b4a4"
   },
   "outputs": [],
   "source": [
    "# Take a peek at the dataset\n",
    "df[\"sentiment\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RTsrFxRWcOE1"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \n",
    "    # Not needed to be imported globally\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    text = BeautifulSoup(text, \"lxml\").get_text()\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE) # Remove urls\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\", ' ', text)\n",
    "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
    "    text = re.sub(r\"[^a-zA-Z.!?']\", ' ', text)\n",
    "    text = re.sub(r\" +\", ' ', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZmQwvcPEckhc",
    "outputId": "0f2862bd-1818-4fe3-867a-69e66bd84d4d"
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda text: preprocess_text(text))\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Gbcp65zz_7wk",
    "outputId": "5eba1187-277d-44b8-9c5c-2951b20bb596"
   },
   "outputs": [],
   "source": [
    "print(\"The number of rows and columns in the dataset is: {}\".format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "h_uP_hOAR_ac",
    "outputId": "e19ced59-0b1b-4431-f4e8-49ae6600c538"
   },
   "outputs": [],
   "source": [
    "# Identify missing values\n",
    "df.apply(lambda x: sum(x.isnull()), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "AelQrBAeAEuJ",
    "outputId": "53f9ff69-ddee-4ea2-e0cc-4b581d808ce4"
   },
   "outputs": [],
   "source": [
    "# Check the target class balance\n",
    "df[\"sentiment\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IE7HnvcfXiUY"
   },
   "source": [
    "**Download token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "rs4wDf4QOwWe",
    "outputId": "13fdeba6-421d-4652-b266-0f26bf588104"
   },
   "outputs": [],
   "source": [
    "!rm -rf bert-base-portuguese-cased\n",
    "!mkdir bert-base-portuguese-cased\n",
    "!wget https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/bert-base-portuguese-cased_pytorch_checkpoint.zip\n",
    "!wget https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/vocab.txt \n",
    "\n",
    "!apt-get install unzip\n",
    "\n",
    "!unzip bert-base-portuguese-cased_pytorch_checkpoint.zip -d bert-base-portuguese-cased\n",
    "!mv vocab.txt bert-base-portuguese-cased/vocab.txt \n",
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel # For BERTs\n",
    "from transformers import AutoModeForSequenceClassification # For models fine-tuned on MNLI\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\") # v1 and v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "UAQ1jHpmXoId",
    "outputId": "91f25992-b18c-43b4-eeb5-e21788d79207"
   },
   "outputs": [],
   "source": [
    "#from transformers import BertTokenizer, BertConfig, TFBertModel\n",
    "#bert_model = TFBertModel.from_pretrained(\"bert-base-portuguese-cased\", from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ESyS2xsQbQlm"
   },
   "outputs": [],
   "source": [
    "# Functions for constructing BERT Embeddings: input_ids, input_masks, input_segments and Inputs\n",
    "MAX_SEQ_LEN=500 # max sequence length\n",
    "\n",
    "def get_masks(tokens):\n",
    "    \"\"\"Masks: 1 for real tokens and 0 for paddings\"\"\"\n",
    "    return [1]*len(tokens) + [0] * (MAX_SEQ_LEN - len(tokens))\n",
    " \n",
    "def get_segments(tokens):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"  \n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (MAX_SEQ_LEN - len(tokens))\n",
    "\n",
    "def get_ids(tokens, tokenizer):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens,)\n",
    "    input_ids = token_ids + [0] * (MAX_SEQ_LEN - len(token_ids))\n",
    "    return input_ids\n",
    "\n",
    "def create_single_input(sentence, tokenizer, max_len):\n",
    "    \"\"\"Create an input from a sentence\"\"\"\n",
    "    stokens = tokenizer.tokenize(sentence)\n",
    "    stokens = stokens[:max_len]\n",
    "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    " \n",
    "    ids = get_ids(stokens, tokenizer)\n",
    "    masks = get_masks(stokens)\n",
    "    segments = get_segments(stokens)\n",
    "\n",
    "    return ids, masks, segments\n",
    " \n",
    "def convert_sentences_to_features(sentences, tokenizer):\n",
    "    \"\"\"Convert sentences to features: input_ids, input_masks and input_segments\"\"\"\n",
    "    input_ids, input_masks, input_segments = [], [], []\n",
    " \n",
    "    for sentence in tqdm(sentences,position=0, leave=True):\n",
    "      ids,masks,segments=create_single_input(sentence,tokenizer,MAX_SEQ_LEN-2)\n",
    "      assert len(ids) == MAX_SEQ_LEN\n",
    "      assert len(masks) == MAX_SEQ_LEN\n",
    "      assert len(segments) == MAX_SEQ_LEN\n",
    "      input_ids.append(ids)\n",
    "      input_masks.append(masks)\n",
    "      input_segments.append(segments)\n",
    "\n",
    "    return [np.asarray(input_ids, dtype=np.int32), \n",
    "          np.asarray(input_masks, dtype=np.int32), \n",
    "          np.asarray(input_segments, dtype=np.int32)]\n",
    "\n",
    "def create_tonkenizer(bert_layer):\n",
    "    \"\"\"Instantiate Tokenizer with vocab\"\"\"\n",
    "    # vocab_file=bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "    # do_lower_case=bert_layer.resolved_object.do_lower_case.numpy() \n",
    "    # tokenizer=bert.bert_tokenization.FullTokenizer(vocab_file,do_lower_case)\n",
    "    do_lower_case = False\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\") # v1 and v2\n",
    "    #tokenizer = BertTokenizer(\"bert-base-portuguese-cased/vocab.txt\", do_lower_case)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xs6_p8VTgpoy"
   },
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C_mFZ2HRcRJB"
   },
   "outputs": [],
   "source": [
    "def nlp_model(callable_object):\n",
    "    # Load the pre-trained BERT base model\n",
    "    # bert_layer = hub.KerasLayer(handle=callable_object, trainable=True)  \n",
    "\n",
    "    bert_layer = callable_object\n",
    "   \n",
    "    # BERT layer three inputs: ids, masks and segments\n",
    "    input_ids = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"input_ids\")           \n",
    "    input_masks = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"input_masks\")       \n",
    "    input_segments = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"segment_ids\")\n",
    "    \n",
    "    inputs = [input_ids, input_masks, input_segments] # BERT inputs\n",
    "    # If using hub.KerasLayer, PLEASE, CHANGE THE ORDER of the variables, I mean: \n",
    "    # pooled_output, sequence_output = \n",
    "    sequence_output, pooled_output = bert_layer(inputs) # BERT outputs \n",
    "    \n",
    "    # Add a hidden layer\n",
    "    x = Dense(units=768, activation='relu')(pooled_output)\n",
    "    x = Dropout(0.3)(x)\n",
    " \n",
    "    # Add output layer\n",
    "    outputs = Dense(3, activation=\"softmax\")(x)\n",
    "\n",
    "    # Construct a new model\n",
    "    model = Model(inputs=inputs, outputs=outputs, )\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "iPQ-WWSidREI",
    "outputId": "954d23fd-6b6b-4246-df25-b12bb0ae80a4"
   },
   "outputs": [],
   "source": [
    "model = nlp_model(bert_model)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ViqJURL5qcZJ"
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "XL5LaS68Qo8G",
    "outputId": "6463c6df-d1be-4394-8d0e-dc817e649e64"
   },
   "outputs": [],
   "source": [
    "# Create examples for training and testing\n",
    "\n",
    "df = df.sample(frac=1) # Shuffle the dataset\n",
    "tokenizer = create_tonkenizer(model.layers[3])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['text'], df['sentiment'], \n",
    "    test_size=0.3, \n",
    "    stratify=df['sentiment'], \n",
    "    random_state=15 \n",
    "    )\n",
    "\n",
    "print( \"\\nx_train: {}; \\tX_test: {}\".format(X_train.shape, X_test.shape))\n",
    "print(\"\\ny_test: \\n{}, \\n\\ny_train: \\n{}\".format(y_train.value_counts(normalize=True), y_test.value_counts(normalize=True) ) )\n",
    "\n",
    "X_train = convert_sentences_to_features(X_train, tokenizer)\n",
    "X_test = convert_sentences_to_features(X_test, tokenizer)\n",
    "\n",
    "y_train = to_categorical( y_train )\n",
    "y_test =  to_categorical( y_test )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "t8CAhbzjnk8M",
    "outputId": "7a6c5c66-9609-43fe-b2cc-af6ee1fe6066"
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rYv9EW9O0rHl"
   },
   "outputs": [],
   "source": [
    "# callback\n",
    "\n",
    "checkpoint_path = \"./sentiment_analysis_model\"\n",
    "ckpt = tf.train.Checkpoint(model=model)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
    "\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        ckpt_manager.save()\n",
    "        print(\"Checkpoint saved at {}.\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "IzSF2HJi5XsY",
    "outputId": "583cfb79-0c49-4f3a-c130-a58cd5118298"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 2\n",
    "\n",
    "# Use Adam optimizer to minimize the categorical_crossentropy loss\n",
    "opt = Adam(learning_rate=2e-5)\n",
    "\n",
    "# loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "# metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "\n",
    "# softmax_cross_entropy_with_logits\n",
    "model.compile(optimizer=opt, \n",
    "              loss= 'categorical_crossentropy', #binary_crossentropy\n",
    "              metrics = ['categorical_accuracy']\n",
    "              )\n",
    "\n",
    "# Fit the data to the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    verbose = 1,\n",
    "                    callbacks=[CustomCallback()]\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_zmw_IyL1172"
   },
   "outputs": [],
   "source": [
    "def save_model(model, name, path, h5=False):\n",
    "  '''\n",
    "  model, model_name, path, h5(optional)\n",
    "  '''\n",
    "  if h5:\n",
    "    !pip install -q pyyaml h5py  # Required to save models in HDF5 format\n",
    "    model.save( \"{}.h5\".format(name) )\n",
    "  else:\n",
    "    model.save( name )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "k4tg3sFSMRv6",
    "outputId": "05f2e467-4937-418f-abf8-aa856c9cd966"
   },
   "outputs": [],
   "source": [
    "save_model(model, \"sentiment_model\", \"trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "QmwK7C4-re9I",
    "outputId": "d7f58567-17a1-4063-857d-0295aebab217"
   },
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "toHq_3_Am6if"
   },
   "source": [
    "## Analysis of model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fcuvU9EhiFXX"
   },
   "outputs": [],
   "source": [
    "# # Load the pretrained nlp_model\n",
    "# from tensorflow.keras.models import load_model\n",
    "# new_model = load_model('test')\n",
    "# new_model.summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KCqF3D54iGiZ"
   },
   "outputs": [],
   "source": [
    "# Predict on test dataset\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "pred_test = np.argmax(model.predict(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "6pcfuN3wvFJV",
    "outputId": "8bb0c103-cccc-47b7-99bc-5d6b6c8a11b7"
   },
   "outputs": [],
   "source": [
    "print(classification_report(np.argmax(y_test,axis=1), pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "ZxyAqz094Lzv",
    "outputId": "74ae77a2-9e3a-4b17-b77a-433d6fb658e7"
   },
   "outputs": [],
   "source": [
    "print(pred_test[:40])\n",
    "print( y_test[:40].argmax(1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iA653kqqM5bk"
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H-tvup1vj-mj"
   },
   "outputs": [],
   "source": [
    "def get_predictions(model_, sentence):\n",
    "  sent = []\n",
    "  sent.append(sentence)\n",
    "  sentence_feature = convert_sentences_to_features(sent, tokenizer)\n",
    "\n",
    "  prediction = np.argmax(model_.predict( sentence_feature ) , axis=1) \n",
    "\n",
    "  # Show Positivo/Negativo\n",
    "  pred = [\"Negativo\" if x == 0 else \"Positivo\" if x == 2 else \"Neutro\"  for x in prediction]\n",
    "\n",
    "  return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "uR4x04shSszC",
    "outputId": "818812de-04a6-4c20-9c74-3df71ad4bb36"
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "get_predictions( model, \"Aquele ator é ruim\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "xaVD-Upun0Pd",
    "outputId": "0dec13bc-e38c-4c26-aa07-ae7cefa2b229"
   },
   "outputs": [],
   "source": [
    "get_predictions( model, \"Eu gosto do seu sorriso\" )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "TensorFlow2_BERT_Sentiment_Analysis.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
